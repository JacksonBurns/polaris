{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "172ae3e5",
   "metadata": {},
   "source": [
    "# Custom datasets and benchmarks\n",
    "We have already seen how easy it is to load a benchmark or dataset from the Polaris Hub. Let's now see how you could create your own!  \n",
    "\n",
    "## Create the dataset\n",
    "\n",
    "A dataset in Polaris is at its core a tabular data-structure in which each row stores a single datapoint. For this example, we will process a multi-task DMPK dataset from [`Fang et al.`](https://doi.org/10.1021/acs.jcim.3c00160). For the sake of simplicity, we don't do any curation and download the dataset as is from their Github.\n",
    "\n",
    "<div class=\"admonition warning highlight\">\n",
    "    <p class=\"admonition-title\">The importance of curation</p>\n",
    "    <p>While we do not address it in this tutorial, data curation is essential to an impactful benchmark. Because of this, we have not just made several high-quality benchmarks readily available on the Polaris Hub, but also open-sourced some of the tools we've built to curate these datasets.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977970cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platformdirs\n",
    "import datamol as dm\n",
    "\n",
    "# We will save the data for this tutorial to our cache dir!\n",
    "SAVE_DIR = dm.fs.join(platformdirs.user_cache_dir(appname=\"polaris-tutorials\"), \"001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "917f6297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PATH = \"https://raw.githubusercontent.com/molecularinformatics/Computational-ADME/main/ADME_public_set_3521.csv\"\n",
    "table = pd.read_csv(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad5c8ec",
   "metadata": {},
   "source": [
    "Since all data fits is contained within the table, creating a dataset is simple. While optional, we will specify some additional meta-data to demonstrate the API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e877f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polaris.dataset import Dataset, ColumnAnnotation\n",
    "\n",
    "dataset = Dataset(\n",
    "    # The table is the core data-structure required to construct a dataset\n",
    "    table=table, \n",
    "    \n",
    "    # All other arguments provide additional meta-data and are optional.\n",
    "    # The exception is the `is_pointer` attribute in the `ColumnAnnotation` object, which\n",
    "    # we will get back to in a later tutorial. \n",
    "    name=\"Fang_2023_DMPK\", \n",
    "    description=\"120 prospective data sets, collected over 20 months across six ADME in vitro endpoints\", \n",
    "    source=\"https://doi.org/10.1021/acs.jcim.3c00160\",\n",
    "    annotations={\n",
    "        \"SMILES\": ColumnAnnotation(modality=\"molecule\"),\n",
    "        \"LOG HLM_CLint (mL/min/kg)\": ColumnAnnotation(user_attributes={\"unit\": \"mL/min/kg\"}),\n",
    "        \"LOG SOLUBILITY PH 6.8 (ug/mL)\": ColumnAnnotation(\n",
    "            protocol=\"Solubility was measured after equilibrium between the dissolved and solid state\"\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146f9a6",
   "metadata": {},
   "source": [
    "## Create the benchmark specification\n",
    "A benchmark is represented by the `BenchmarkSpecification`, which wraps a `Dataset` with additional data to produce a benchmark. \n",
    "\n",
    "Specifically, it specifies:\n",
    "1. Which dataset to use (see Dataset);\n",
    "2. Which columns are used as input and which columns are used as target;\n",
    "3. Which metrics should be used to evaluate performance on this task;\n",
    "4. A predefined, static train-test split to use during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3313a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from polaris.benchmark import SingleTaskBenchmarkSpecification\n",
    "\n",
    "# For the sake of simplicity, we use a very simple, ordered split\n",
    "split = (\n",
    "    np.arange(3000).tolist(), # train\n",
    "    (np.arange(521) + 3000).tolist()  # test\n",
    ")\n",
    "\n",
    "benchmark = SingleTaskBenchmarkSpecification(\n",
    "    dataset=dataset, \n",
    "    target_cols=\"LOG SOLUBILITY PH 6.8 (ug/mL)\",\n",
    "    input_cols=\"SMILES\",\n",
    "    split=split,\n",
    "    metrics=\"mean_absolute_error\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290672ff",
   "metadata": {},
   "source": [
    "Metrics should be supported in the polaris framework\n",
    "For more information, see the `Metric` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "103f73d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy', 'mean_absolute_error', 'mean_squared_error']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from polaris.evaluate import Metric\n",
    "\n",
    "Metric.list_supported_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ee4c2",
   "metadata": {},
   "source": [
    "To support to vast flexibility in specifying a benchmark, we have different classes that correspond to different types of benchmarks. Each of these sub-classes make the data-model or logic more specific to a particular case. For example, trying to create a multi-task benchmark with the same arguments will throw an error as there is just a single target column specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff3cce88",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for MultiTaskBenchmarkSpecification\ntarget_cols\n  Value error, A multi-task benchmark should specify at least two target columns [type=value_error, input_value='LOG SOLUBILITY PH 6.8 (ug/mL)', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.1.2/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpolaris\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbenchmark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiTaskBenchmarkSpecification\n\u001b[0;32m----> 3\u001b[0m benchmark \u001b[38;5;241m=\u001b[39m \u001b[43mMultiTaskBenchmarkSpecification\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLOG SOLUBILITY PH 6.8 (ug/mL)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSMILES\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean_absolute_error\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/local/conda/envs/polaris/lib/python3.11/site-packages/pydantic/main.py:150\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    149\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m \u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__pydantic_self__\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for MultiTaskBenchmarkSpecification\ntarget_cols\n  Value error, A multi-task benchmark should specify at least two target columns [type=value_error, input_value='LOG SOLUBILITY PH 6.8 (ug/mL)', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.1.2/v/value_error"
     ]
    }
   ],
   "source": [
    "from polaris.benchmark import MultiTaskBenchmarkSpecification\n",
    "\n",
    "benchmark = MultiTaskBenchmarkSpecification(\n",
    "    dataset=dataset, \n",
    "    target_cols=\"LOG SOLUBILITY PH 6.8 (ug/mL)\",\n",
    "    input_cols=\"SMILES\",\n",
    "    split=split,\n",
    "    metrics=\"mean_absolute_error\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a036795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try that again, but now with two target columns\n",
    "benchmark = MultiTaskBenchmarkSpecification(\n",
    "    dataset=dataset, \n",
    "    target_cols=[\"LOG SOLUBILITY PH 6.8 (ug/mL)\", \"LOG HLM_CLint (mL/min/kg)\"],\n",
    "    input_cols=\"SMILES\",\n",
    "    split=split,\n",
    "    metrics=\"mean_absolute_error\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1bae4e",
   "metadata": {},
   "source": [
    "## Save the benchmark\n",
    "Saving the benchmark is easy and can be done with a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adbaf57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = benchmark.to_json(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46ed8f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/cas/.cache/polaris-tutorials/001/benchmark.json',\n",
       " '/home/cas/.cache/polaris-tutorials/001/dataset.json',\n",
       " '/home/cas/.cache/polaris-tutorials/001/table.parquet']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = dm.fs.get_mapper(SAVE_DIR).fs\n",
    "fs.ls(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a691ae",
   "metadata": {},
   "source": [
    "This created three files. Two `json` files and a single `parquet` file. The `parquet` file saves the tabular structure at the base of the `Dataset` class, whereas the `json` files save all the meta-data for the `Dataset` and `BenchmarkSpecification`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac20686",
   "metadata": {},
   "source": [
    "## Load the benchmark\n",
    "Loading the benchmark is easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f1dd431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import polaris as po\n",
    "\n",
    "benchmark = po.load_benchmark(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123165c5",
   "metadata": {},
   "source": [
    "## Use the benchmark\n",
    "\n",
    "Using your custom benchmark is seamless. It supports the exact same API as any benchmark that would be loaded through the hub:\n",
    "\n",
    "1. `get_train_test_split()`: For creating objects through which we can access the different dataset partitions. \n",
    "2. `evaluate()`: For evaluating a set of predictions in accordance with the benchmark protocol. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52898f06",
   "metadata": {},
   "source": [
    "### Data access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "680d3ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = benchmark.get_train_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca4e8f",
   "metadata": {},
   "source": [
    "The created objects support various flavours to access the data.\n",
    "1. The objects are iterable;\n",
    "2. The objects can be indexed; \n",
    "3. The objects have properties to access all data at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95e58560",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x, y in train: \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da7cc310",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    x, y = train[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26409957",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train.inputs\n",
    "y = train.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728906df",
   "metadata": {},
   "source": [
    "To avoid accidental access to the test targets, the test object does not expose the labels and will throw an error if you try access them explicitly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48b461fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in test: \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3a6cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test)):\n",
    "    x = test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bfc4be5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TestAccessError",
     "evalue": "Within Polaris, you should not need to access the targets of the test set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTestAccessError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39minputs\n\u001b[0;32m----> 2\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargets\u001b[49m\n",
      "File \u001b[0;32m~/Documents/repositories/polaris/polaris/dataset/_subset.py:103\u001b[0m, in \u001b[0;36mSubset.targets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtargets\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    Scikit-learn style access to the targets.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    If the dataset is multi-target, this will return a dict of arrays.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/repositories/polaris/polaris/dataset/_subset.py:139\u001b[0m, in \u001b[0;36mSubset.as_array\u001b[0;34m(self, data_type)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_array(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_array(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hide_targets:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TestAccessError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWithin Polaris, you should not need to access the targets of the test set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_multi_task:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract(ret, data_type) \u001b[38;5;28;01mfor\u001b[39;00m ret \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m])\n",
      "\u001b[0;31mTestAccessError\u001b[0m: Within Polaris, you should not need to access the targets of the test set"
     ]
    }
   ],
   "source": [
    "x = test.inputs\n",
    "y = test.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c2839c",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "To evaluate a set of predictions within Polaris, you should use the `evaluate()` endpoint.This requires you to just provide the predictions. The targets of the test set are automatically extract so that the chance of the user accessing the test labels is minimal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30696dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have a multi-task dataset, we should provide predictions for both targets\n",
    "y_pred = {\n",
    "    \"LOG SOLUBILITY PH 6.8 (ug/mL)\": np.random.random(len(test)),\n",
    "    \"LOG HLM_CLint (mL/min/kg)\": np.random.random(len(test)),\n",
    "}\n",
    "\n",
    "results = benchmark.evaluate(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd07968",
   "metadata": {},
   "source": [
    "The resulting object does not just store the results, but also allows for additional meta-data that can be uploaded to the hub. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7f73023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOG SOLUBILITY PH 6.8 (ug/mL)': {'mean_absolute_error': 0.9311836043999315},\n",
       " 'LOG HLM_CLint (mL/min/kg)': {'mean_absolute_error': 0.7306570958492925}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84854b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 7, 20, 16, 36, 20, 24731)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results._created_at"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e7fd17",
   "metadata": {},
   "source": [
    "This will currently fail as we do not have a client (or a Hub for that matter) yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a89ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.upload_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9cac38",
   "metadata": {},
   "source": [
    "The End. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
